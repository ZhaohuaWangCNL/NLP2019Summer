{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    skill sets including python, machine learning, statistics etc.\n",
    "    \n",
    "    2.2. what problems do you want to solve？\n",
    "    If we are talking about goals, then it is to launch a job as an algorithm engineer or data scientist eventually.\n",
    "    \n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    The capability of learning fast, already-acquired skills like English reading and writing, as well as background knowledge I got during my bachelor's study(although a bit rusty) could help me to accomplish my goal. \n",
    "    \n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    My weak foundation in computer science, for example, it took me quite some time to have an idea of what pip was for lol\n",
    "    \n",
    "    2.5. How will you plan to study in this course period?\n",
    "    Besides of the lectures, I will set aside at least 4 hours a day for this course so I could keep up with the lectures, homework as well as getting familiar with any basics related.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {AI-enabled customer assistant/alipay chatbox etc;\n",
    "finance advisor giving advice based on algorithms and data analysing;\n",
    "Amazon drone delivery}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {\n",
    "Github  ==> Sign up -> create repository for our own projects / clone respository of Computing-Intelligence to get course resources / upload and save code/documents under our own account\n",
    "Jupyter and Pycharm==> with Jupyter we can create and share documents and include live code, equations,jpg etc, and hence it is a good way to present. However it could be really slow when we are using it to handle big data, that's where Pycharm comes in as it is quite friendly for development.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: It describes chance process, i.e. the probability of a certain event in a sample space. For example, if you toss a coin, you might get head or tail, and the chance of each is 1/2. That is, probability(head)=1/2, probability(tail)=1/2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:I will give an example of rish and probability design in the Netherlands. As a country that lies on North Sea and most of the land is close to sea level,the Dutch have put a lot of effort in building and maintaining their dam. However,the engineers have to decide how strong the dam should be considering risks, which is closely related to the probability of flooding. You don't want spend a huge amount of money to build a dam that could protect you from storm that happens once a billion year. Neither do you build a dam that is so weak that can't stand a flooding that we encount every 2 years. As an engineer, you determin NPV related to probability of flooding to make a decision.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:The difficuly for pasing and pattern match lies in the complexity of language. When we use probability, we shift it from a linguistic problem to a mathematic problem to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Language Model estimates the relative likelihood of sequence of words. It can be taken as a function of x,where x is a sentence/or to be more accurately a sequence of words, and the result is the probability of the sequence of words, the value of which is hence between 0 and 1, and it is moe likely if it is close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:When we design robots and try to make them understand us and sound like human, say they could understand what we say and respond corespondingly like a human being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Basically 1-gram means 1-word. Therefore, in the 1-gram language model, what counts is the probability of a single word without any context, i.e. the probability of a certain word only depends on its own probability in a certain document. For example,in a 1-gram LM, the probability of the sentence 'I am doing homework' is:\n",
    "P(I,am,doing,homework)=P(I)P(am)P(doing)P(homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:In a unigram language model, the order/sequence of words doesn't matter, but instead the counts of a single word matters. So it can tell information like which word features in, say, a certain document, and a typical application is spam filtering. However, 1-gram LM won't perform well in checking the likelihood of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:2-gram means 2-words, or to say, the probability of a word in a document depends on the probability of the word in front (or behind). \n",
    "For example,in a 2-gram LM, the probability of the sentence 'I am doing homework' is approximately:\n",
    "P(I,am,doing,homework) ~  P(I|am)P(am|doing)P(doing|homework)P(homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = '''\n",
    "sentence = 寒暄 ， 询问主语 询问主体 结尾\n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ，\n",
    "人称 = 先生|女士|小朋友\n",
    "打招呼 = 你好|您好\n",
    "询问主语 = 请问你 | 请问您 |请问你家人\n",
    "询问主体 = 住址 | 身份证号 |名字 | 年龄 | 驾照号码\n",
    "结尾 = 是？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost = '''\n",
    "sentence = 主语 主体 答案\n",
    "主语 = 自己 相关\n",
    "自己 = 我 | 俺\n",
    "相关 = null | 妈妈 | 家人 | 爱人 | 爸爸 | 儿子\n",
    "主体 = 地址 | 身份证 | 驾照 | null\n",
    "答案 = 在南京 | 记不得 | 叫王朝华 | 34 | 没有\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好，请问你名字是？'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def create_grammar(grammar_str, split = '=', line_split = '\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp,stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar\n",
    "def generate(gram,target):\n",
    "    if target not in gram: return target\n",
    "    expanded = [generate(gram,t) for t in random.choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expanded if e != 'null'])\n",
    "generate(gram=create_grammar(helper),target='sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "俺妈妈身份证叫王朝华\n",
      "俺家人在南京\n",
      "我爱人身份证没有\n",
      "俺爸爸记不得\n",
      "俺爱人身份证记不得\n",
      "俺儿子身份证记不得\n",
      "俺爸爸没有\n",
      "我爸爸叫王朝华\n",
      "我家人驾照记不得\n",
      "我儿子驾照34\n"
     ]
    }
   ],
   "source": [
    "def generate_n(n):\n",
    "    for i in range(n):\n",
    "        print (generate(create_grammar(lost),target='sentence')) \n",
    "    pass\n",
    "generate_n(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'很真实的感觉.像纪录片\\r\\n觉得姑娘很傻很天真\\r\\n父母很烦很无奈很现实\\r\\n生活就是现实的'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "filename = '/Users/wangz/Documents/GitHub/datasource/movie_comments.csv'\n",
    "import pandas as pd\n",
    "content = pd.read_csv(filename, encoding='utf_8')\n",
    "content.head()\n",
    "articles = content['comment'].tolist()\n",
    "articles[260001]  #check with a random line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很真实的感觉像纪录片觉得姑娘很傻很天真父母很烦很无奈很现实生活就是现实的'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    return re.findall('\\w+', string)\n",
    "articles_clean = [''.join(token(str(a))) for a in articles]\n",
    "articles_clean[260001]  #check with a random line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pypi.douban.com/simple\n",
      "Requirement already satisfied: jieba in c:\\programdata\\anaconda3\\lib\\site-packages (0.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wangz\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.091 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['吴京',\n",
       " '意淫',\n",
       " '到',\n",
       " '了',\n",
       " '脑残',\n",
       " '的',\n",
       " '地步',\n",
       " '看',\n",
       " '了',\n",
       " '恶心',\n",
       " '想',\n",
       " '吐',\n",
       " '\\n',\n",
       " '首映礼',\n",
       " '看',\n",
       " '的',\n",
       " '太',\n",
       " '恐怖',\n",
       " '了',\n",
       " '这个',\n",
       " '电影',\n",
       " '不讲道理',\n",
       " '的',\n",
       " '完全',\n",
       " '就是',\n",
       " '吴京',\n",
       " '在',\n",
       " '实现',\n",
       " '他',\n",
       " '这个',\n",
       " '小',\n",
       " '粉红',\n",
       " '的',\n",
       " '英雄',\n",
       " '梦',\n",
       " '各种',\n",
       " '装备',\n",
       " '轮番',\n",
       " '上场',\n",
       " '视',\n",
       " '物理',\n",
       " '逻辑',\n",
       " '于',\n",
       " '不顾',\n",
       " '不得不',\n",
       " '说',\n",
       " '有钱',\n",
       " '真',\n",
       " '好',\n",
       " '随意',\n",
       " '胡闹',\n",
       " '\\n',\n",
       " '吴京',\n",
       " '的',\n",
       " '炒作',\n",
       " '水平',\n",
       " '不输',\n",
       " '冯小刚',\n",
       " '但小刚',\n",
       " '至少',\n",
       " '不会',\n",
       " '用',\n",
       " '主旋律',\n",
       " '来',\n",
       " '炒作',\n",
       " '吴京',\n",
       " '让',\n",
       " '人',\n",
       " '看',\n",
       " '了',\n",
       " '不',\n",
       " '舒服',\n",
       " '为了',\n",
       " '主旋律',\n",
       " '而',\n",
       " '主旋律',\n",
       " '为了',\n",
       " '煽情',\n",
       " '而',\n",
       " '煽情',\n",
       " '让',\n",
       " '人',\n",
       " '觉得',\n",
       " '他',\n",
       " '是',\n",
       " '个',\n",
       " '大',\n",
       " '做作',\n",
       " '大',\n",
       " '谎言',\n",
       " '家',\n",
       " '729',\n",
       " '更新',\n",
       " '片子',\n",
       " '整体',\n",
       " '不如',\n",
       " '湄公河',\n",
       " '行动',\n",
       " '1',\n",
       " '整体']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('article.txt', 'w',encoding='utf_8') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')\n",
    "!pip install jieba\n",
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "TOKEN = []\n",
    "for i, line in enumerate((open('article.txt',encoding='utf_8'))):\n",
    "    TOKEN += cut(line)\n",
    "TOKEN[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words_count = Counter(TOKEN)\n",
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1/len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.343268699453012e-40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def get_probablity(sentence):\n",
    "    sentence_clean = ''.join(re.findall('\\w+',str(sentence)))\n",
    "    words = cut(sentence_clean)\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 我妈妈34 with Prb: 1.0629026087671023e-12\n",
      "sentence: 我爱人地址在南京 with Prb: 1.765253055818612e-26\n",
      "sentence: 我爱人驾照没有 with Prb: 2.796047864221109e-20\n",
      "sentence: 我爱人地址在南京 with Prb: 1.765253055818612e-26\n",
      "sentence: 俺爱人驾照没有 with Prb: 9.320159547403693e-21\n",
      "sentence: 俺爸爸驾照记不得 with Prb: 9.320159547403693e-21\n",
      "sentence: 我爸爸驾照叫王朝华 with Prb: 6.604281264707333e-33\n",
      "sentence: 俺没有 with Prb: 2.104462133150161e-07\n",
      "sentence: 俺爸爸地址记不得 with Prb: 9.320159547403693e-21\n",
      "sentence: 俺爱人地址记不得 with Prb: 9.320159547403693e-21\n"
     ]
    }
   ],
   "source": [
    "example_grammar = lost\n",
    "for sen in [generate(gram=create_grammar(example_grammar),target='sentence') for i in range(10)]:\n",
    "    print('sentence: {} with Prb: {}'.format(sen, get_probablity(sen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(example_grammar,language_model): \n",
    "    n=10000\n",
    "    list_sen=[]\n",
    "    for sen in [generate(gram=create_grammar(example_grammar),target='sentence') for i in range(n)]:\n",
    "        if language_model == '2-gram':\n",
    "            list_sen.append([sen, get_probablity(sen)])\n",
    "    return sorted(list_sen,key=lambda x:x[1],reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，请问您名字是？'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(helper,'2-gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:The grammar 'helper' generates more humanable sentence while sentences generated by the grammmar 'lost' sounds a bit ridiculus (although it sounds like someone who is lost and actually it was designed for human in the human(lost)-robot(helper） scinario. Improving the definition of the grammar could help.\n",
    "Secondly, the size of database to train the 2-gram language model could be bigger.\n",
    "Third, when world1 and word2 do not appear together in the provided training database, 1/len(TOKEN_2_GRAM) was taken as prob_2, which I think is not the most ideal approximation we can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
